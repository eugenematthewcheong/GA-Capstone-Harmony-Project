{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project: Harmony\n",
    "## 1.5 Consolidate Product Database\n",
    "> Authors: Eugene Matthew Cheong\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents ##\n",
    "\n",
    "#### 1. Web Scraping\n",
    "\n",
    "- [1.1 Scraping Lian Seng Hin Website](1.1_web_scraping_liansenghin.ipynb)\n",
    "- [1.2 Scraping Hafary Website](1.2_web_scraping_hafary.ipynb)\n",
    "- [1.3 Scraping Lamitak Website](1.3_web_scraping_lamitak.ipynb)\n",
    "- [1.4 Scraping Nippon Website](1.4_web_scraping_nippon.ipynb)\n",
    "- [1.5 Consolidate All Product Database](1.5_consolidate_product_database.ipynb)\n",
    "\n",
    "#### 2. Preprocessing\n",
    "\n",
    "- [2.1 Processing Canva Palettes](2.1_processing_canva_palette.ipynb)\n",
    "\n",
    "#### 3. Modelling\n",
    "\n",
    "- [3.1 Matching Input Photo to Products](3.1_matching_input_photo_to_products.ipynb)\n",
    "- [3.2 Recommending Canva Palette to Products](3.2_recommending_canva_palette_to_product.ipynb)\n",
    "- [3.3 Recommending Colours and Colour Palettes with Llama3](3.3_recommending_colours_and_colour_palettes_with_llama3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2hsv, hsv2rgb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import euclidean, cosine, cityblock\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for saving and loading h5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_hdf5(processed_images, filename):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        for i, (img, flat) in enumerate(processed_images):\n",
    "            f.create_dataset(f'image_{i}', data=img)\n",
    "            f.create_dataset(f'flat_{i}', data=flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_hdf5(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Initialize containers for images and flattened data\n",
    "        images = []\n",
    "        flattened_data = []\n",
    "\n",
    "        # Iterate over items in HDF5 file and load them\n",
    "        for i in range(len(f.keys()) // 2):  # Assuming each image has two corresponding keys (image and flat)\n",
    "            img_key = f'image_{i}'\n",
    "            flat_key = f'flat_{i}'\n",
    "            \n",
    "            # Load and append to respective lists\n",
    "            images.append(np.array(f[img_key]))  # Convert to numpy array if necessary\n",
    "            flattened_data.append(np.array(f[flat_key]))\n",
    "\n",
    "    return images, flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and cleaning up all the catalogue dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List to reorder the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Model Name', 'Company', 'Type', 'Origin Country', 'Application', 'Filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hafary_df = pd.read_csv('../datasets/hafary_df.csv').drop(['Unnamed: 0'],axis=1).drop(columns=['Height (cm)', 'Width (cm)', 'Product URL', 'Category Tags'])\n",
    "lamitak_df = pd.read_csv('../datasets/lamitak_df.csv').drop(['Unnamed: 0'],axis=1).drop(columns=['Height (cm)', 'Width (cm)', 'Product URL', 'Category Tags'])\n",
    "liansenghin_df = pd.read_csv('../datasets/liansenghin_df.csv').drop(['Unnamed: 0'],axis=1).drop(columns=['Height (cm)', 'Width (cm)', 'Product URL', 'Category Tags'])\n",
    "nippon_df = pd.read_csv('../datasets/nippon_df.csv').drop(['Unnamed: 0'],axis=1).drop(columns=['Product URL', 'Category Tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 catalogues did not have information of where the products were made, so I decided to fill it in it with 'Singapore' for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hafary_df = hafary_df[columns]\n",
    "\n",
    "lamitak_df = lamitak_df[columns]\n",
    "lamitak_df['Origin Country'] = lamitak_df['Origin Country'].fillna('Singapore')\n",
    "\n",
    "liansenghin_df = liansenghin_df[columns]\n",
    "\n",
    "nippon_df = nippon_df[columns]\n",
    "nippon_df['Origin Country'] = lamitak_df['Origin Country'].fillna('Singapore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamitak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liansenghin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nippon_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and removing missing data in nippon_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was some errors that were caused when using the recommendation system and noticed that there were missing values in the dataframe. Based on observations, it looks like the scraping function could not pick up the Model name for some products. Will be removing them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nippon_df['Model Name'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nippon_df['Filename'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nippon_df = nippon_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were images that were named 'nan.png' as well. Will be removing them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nippon_df = nippon_df[nippon_df['Filename'] != 'nan.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nippon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nippon_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined all the dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_df = pd.concat([hafary_df, liansenghin_df, lamitak_df, nippon_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export combined products to \"all_products_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_df.to_csv('../datasets/all_products_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import \"all_products_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_df = pd.read_csv('../datasets/all_products_df.csv')\n",
    "image_folder = '../datasets/images/all_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_products_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing all the images so that it'll be easier to load in later on than calculating individual images every time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and flatten images\n",
    "def preprocess(image_path):\n",
    "    img = imread(image_path)\n",
    "    if  img.shape[2] == 4:  # Check if the image has an alpha channel\n",
    "        img = img[:, :, :3]  # Remove the alpha channel if present\n",
    "\n",
    "    # Convert the RGB image to Lab color space\n",
    "    #img_lab = rgb2hsv(img)\n",
    "    img_lab = rgb2lab(img)\n",
    "\n",
    "    # Resize the image to a fixed size (e.g., 256x256)\n",
    "    img_resized = resize(img_lab, (256, 256), anti_aliasing=True)\n",
    "    \n",
    "    # Optionally, you can flatten the image if needed for further processing\n",
    "    img_flattened = img_resized.flatten()\n",
    "\n",
    "    # Return both the resized Lab image and the flattened version\n",
    "    return img_resized, img_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_cosine(u, v):\n",
    "    if np.linalg.norm(u) == 0 or np.linalg.norm(v) == 0:\n",
    "        return 1.0  # Use 1.0 to indicate maximum dissimilarity or undefined similarity\n",
    "    else:\n",
    "        return cosine(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "\n",
    "for i in list(all_products_df['Filename']):\n",
    "  full_image_filepath = os.path.join(image_folder,i)\n",
    "  if os.path.exists(full_image_filepath):\n",
    "    image_list.append(os.path.join(image_folder,i))\n",
    "  else:\n",
    "    print(f\"Error finding image path: {full_image_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = [preprocess(img_path) for img_path in image_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving preprocessed all_images to a h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_hdf5(processed_images, os.path.join(data_folder,'h5','preprocessed_all_images.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Notebook: [2.1 Processing Canva Palettes](2.1_processing_canva_palette.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
