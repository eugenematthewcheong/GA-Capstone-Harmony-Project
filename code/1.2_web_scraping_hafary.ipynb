{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef95eb8",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project: Harmony\n",
    "## 1.2 Web scraping - Hafary\n",
    "> Authors: Eugene Matthew Cheong\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8393dace",
   "metadata": {},
   "source": [
    "## Table of Contents ##\n",
    "\n",
    "#### 1. Web Scraping\n",
    "\n",
    "- [1.1 Scraping Lian Seng Hin Website](1.1_web_scraping_liansenghin.ipynb)\n",
    "- [1.2 Scraping Hafary Website](1.2_web_scraping_hafary.ipynb)\n",
    "- [1.3 Scraping Lamitak Website](1.3_web_scraping_lamitak.ipynb)\n",
    "- [1.4 Scraping Nippon Website](1.4_web_scraping_nippon.ipynb)\n",
    "- [1.5 Consolidate All Product Database](1.5_consolidate_product_database.ipynb)\n",
    "\n",
    "#### 2. Preprocessing\n",
    "\n",
    "- [2.1 Processing Canva Palettes](2.1_processing_canva_palette.ipynb)\n",
    "\n",
    "#### 3. Modelling\n",
    "\n",
    "- [3.1 Matching Input Photo to Products](3.1_matching_input_photo_to_products.ipynb)\n",
    "- [3.2 Recommending Canva Palette to Products](3.2_recommending_canva_palette_to_product.ipynb)\n",
    "- [3.3 Recommending Colours and Colour Palettes with Llama3](3.3_recommending_colours_and_colour_palettes_with_llama3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e2a14",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a57182",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994bbbb-74ef-49f6-9823-282217604394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a270d",
   "metadata": {},
   "source": [
    "# Website to scrape\n",
    "\n",
    "- https://www.hafary.com.sg/collections/Tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512e214-26d2-4902-bf8b-7ca12ec1110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_img_folder = \"../datasets/images\"\n",
    "hafary_img_folder =  os.path.join(data_img_folder,\"hafary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c431988",
   "metadata": {},
   "source": [
    "# Scraping for Hafary tile products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f1087",
   "metadata": {},
   "source": [
    "### Function to scrape information required per Hafary page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9caa47-8313-4830-99b0-cf805fac493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape images and labels from a single page\n",
    "def scrape_page(url,input_folder,typeofproduct):\n",
    "\n",
    "    # Create a directory to store images\n",
    "    os.makedirs(input_folder, exist_ok=True)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "    # Find image containers within the specified class\n",
    "    image_containers = soup.find('div', class_=\"div_global_right_content collection\").find_all('a')\n",
    "\n",
    "    product_list = []\n",
    "    # Extract image URLs and labels\n",
    "    for container in image_containers:\n",
    "        \n",
    "        product_site = f\"https://www.hafary.com.sg{container['href']}\"\n",
    "\n",
    "        image_label = container.find('div', class_=\"div_global_grid_title\").text.strip()\n",
    "\n",
    "        image_tags = container.find_all('div', class_=\"div_global_grid_image_collection\")\n",
    "        for image in image_tags:\n",
    "            image_url = image.find('img')['src']\n",
    "            image_url_correct = f\"https://www.hafary.com.sg{image_url}\"\n",
    "\n",
    "            image_label_clean = image_label.replace('.','-').replace(' ','-').replace('/','-').lower()\n",
    "\n",
    "            #Remove file extension\n",
    "            image_basename = os.path.basename(image_url)\n",
    "            image_name = os.path.splitext(image_basename)[0]\n",
    "            image_filename = f\"{image_label_clean}_{image_name}\"\n",
    "            \n",
    "            \n",
    "            image_dict = {\"Model Name\": image_label, \n",
    "                          \"Product URL\": product_site, \n",
    "                          \"Filename\": f\"{image_filename}.jpg\", \n",
    "                          \"Company\" : \"Hafary\",\n",
    "                          \"Type\" : typeofproduct}\n",
    "            \n",
    "            #Download image and save with label\n",
    "            download_image(image_url_correct, image_filename, input_folder)\n",
    "            product_list.append(image_dict)\n",
    "            \n",
    "            print(f\"Image Label Found: {image_label}\")\n",
    "            print(f\"Image Product Site Found: {product_site}\")\n",
    "            print(f\"Image URL Found: {image_url_correct}\")\n",
    "    \n",
    "    return product_list       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd818c",
   "metadata": {},
   "source": [
    "### Function to download images with given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download image and save with label\n",
    "def download_image(url, label, input_folder):\n",
    "    image_data = requests.get(url).content\n",
    "    filename = f\"{label}.jpg\"\n",
    "    image_filepath = os.path.join(input_folder,filename)\n",
    "    with open(image_filepath, 'wb') as f:\n",
    "        f.write(image_data)\n",
    "    print(f\"Image saved: {image_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f3bfe",
   "metadata": {},
   "source": [
    "### Function to scrape Hafary pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1a945-61dc-4cf5-b7b7-61604c3f77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to iterate through pages and scrape\n",
    "def hafary_main(base_url, typeofproduct):\n",
    "    page_number = 1\n",
    "\n",
    "    all_product_list = []\n",
    "    # Iterate through pages\n",
    "    #while page_number <= page_limit:\n",
    "    while True:\n",
    "        page_url = f\"{base_url}?Page={page_number}/\"\n",
    "        response = requests.get(page_url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"No more pages. Exiting.\")\n",
    "            break\n",
    "        \n",
    "        #Find if there are anymore content of interest in the page, else break.\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        if soup.find('div', class_=\"div_global_right_content collection\").find_all('a') == []:\n",
    "            print(f\"No more pages. Exiting.\")\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        print(f\"Scraping page {page_number}...\")\n",
    "        product_list_page = scrape_page(page_url,hafary_img_folder, typeofproduct)\n",
    "        all_product_list += product_list_page\n",
    "        page_number += 1\n",
    "    \n",
    "    return all_product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bad56d-05c3-47ef-98a8-c31d8f366eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "base_url = \"https://www.hafary.com.sg/collections/Tiles\"\n",
    "all_product_list = hafary_main(base_url, \"Tiles\")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Scraping Runtime:\", runtime, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31b731",
   "metadata": {},
   "source": [
    "Converting to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a7170-a497-4fcc-afbd-c5ef8d599a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.DataFrame(all_product_list)\n",
    "product_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82f387",
   "metadata": {},
   "source": [
    "Checking if there are any null values in the 'Product URL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['Product URL'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32477df7",
   "metadata": {},
   "source": [
    "### Function to populate tile details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1afcd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_tile_details(row):\n",
    "   print(\"Processing row:\", row.name)\n",
    "   product_url = row['Product URL']\n",
    "\n",
    "\n",
    "   response = requests.get(product_url)\n",
    "   if response.status_code != 200:\n",
    "      print(f\"Failed to fetch {product_url}\")\n",
    "      return None\n",
    "\n",
    "   soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "   # Find image containers within the specified class\n",
    "   info_containers = soup.find('div', \"div_more_info\").find_all('div', class_= 'column')\n",
    "\n",
    "   all_dict = {}\n",
    "   # Extract image URLs and labels\n",
    "   for container in info_containers:\n",
    "      product_headers = container.find_all(\"div\", class_=\"div_microsite_sub_title_header\")\n",
    "      product_descriptions = container.find_all(\"div\", class_=\"div_microsite_sub_title\")\n",
    "\n",
    "      product_headers_list = []\n",
    "      product_descriptions_list = []\n",
    "\n",
    "      for header in product_headers:\n",
    "         product_headers_list.append(header.text.strip())\n",
    "\n",
    "      for descriptions in product_descriptions:\n",
    "         product_descriptions_list.append(descriptions.text.strip())\n",
    "\n",
    "      all_dict.update(dict(zip(product_headers_list, product_descriptions_list)))\n",
    "\n",
    "   product_description = \"\"\n",
    "\n",
    "   try:\n",
    "      product_description += f\"{all_dict['MATERIAL']}\"\n",
    "   except:\n",
    "      print(f\"Error: No Materials Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "   try:\n",
    "      product_description += f\", {all_dict['FEATURES']}\"\n",
    "   except:\n",
    "      print(f\"Error: No Features Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "   try:\n",
    "      product_description += f\", {all_dict['VARIATION']}\"\n",
    "   except:\n",
    "      print(f\"Error: No Variations Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "\n",
    "   # Split the text using regular expression\n",
    "   application_split_text_list = re.findall('[A-Z][^A-Z]*', all_dict['APPLICATION'])\n",
    "   application_split_text = \", \".join(application_split_text_list)\n",
    "\n",
    "   try:\n",
    "      dimension_text = all_dict['AVAILABLE DIMENSIONS'].replace(\"(L)\",\"\").replace(\"(W)\",\"\").replace(\"mm\",\"\").replace(\" \",\"\")\n",
    "      dimension_list = dimension_text.split(\"×\")\n",
    "      row['Width (cm)'] = dimension_list[0]\n",
    "      row['Height (cm)'] = dimension_list[1]\n",
    "   except:\n",
    "      print(f\"Error: No Dimensions Found for {row['Product URL']}\")\n",
    "      pass  \n",
    "\n",
    "   try:\n",
    "      row['Origin Country'] = all_dict['COUNTRY OF ORIGIN']\n",
    "   except:\n",
    "      print(f\"Error: No Country of Origin Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "   try:\n",
    "      row['Application'] = application_split_text\n",
    "   except:\n",
    "      print(f\"Error: No Variations Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "         \n",
    "   row['Category Tags'] = product_description\n",
    "   row['Type'] = \"Tiles\"\n",
    "\n",
    "   return row\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "product_df =  product_df.apply(populate_tile_details, axis=1).dropna()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Scraping Runtime:\", runtime, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980b9ad",
   "metadata": {},
   "source": [
    "There are special characters like * in the naming of the file. Needs to be renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa410190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(df, folder):\n",
    "    # Iterate over the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        original_filename = row['Filename']\n",
    "        # Check if '*' is in the filename\n",
    "        if '*' in original_filename or '\\\\' in original_filename:\n",
    "            # Replace '*' with '-'\n",
    "            new_filename = original_filename.replace('*', '-').replace('\\\\', '-')\n",
    "            # Update the DataFrame\n",
    "            df.at[index, 'Filename'] = new_filename\n",
    "            # Rename the file on disk\n",
    "            original_filepath = os.path.join(folder, original_filename)\n",
    "            new_filepath = os.path.join(folder, new_filename)\n",
    "            if os.path.exists(original_filepath):\n",
    "                os.rename(original_filepath, new_filepath)\n",
    "            else:\n",
    "                print(f\"File does not exist: {original_filepath}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97815057",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = rename_files(product_df, hafary_img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['Filename'].str.contains(r'\\*', regex=True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09870201",
   "metadata": {},
   "source": [
    "# Export Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3d9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_dataset_path = \"../datasets/archive_dataset/\"\n",
    "file_path = '../datasets/hafary_df.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757799ac",
   "metadata": {},
   "source": [
    "Archives the old csv and updates with the current list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(archive_dataset_path):\n",
    "    os.makedirs(archive_dataset_path)  # Create the archive folder if it doesn't exist\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Move the file to the archive folder\n",
    "    shutil.move(file_path, os.path.join(archive_dataset_path, f\"hafary_df_archived_{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df.to_csv(\"../datasets/hafary_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294c5d4",
   "metadata": {},
   "source": [
    "I noticed after scraping, there are some images that are not correct and showing the tile image. It shows a room instead. So I will update the images later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a8713",
   "metadata": {},
   "source": [
    "# Find missing files and update to the correct image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f14547",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv(\"../datasets/hafary_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ee3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_image_list = []\n",
    "\n",
    "for i in list(product_df['Filename']):\n",
    "  full_image_filepath = os.path.join(hafary_img_folder,i)\n",
    "  if os.path.exists(full_image_filepath):\n",
    "    missing_image_list.append(os.path.join(hafary_img_folder,i))\n",
    "  else:\n",
    "    print(f\"Error finding image path: {full_image_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e9b7b",
   "metadata": {},
   "source": [
    "# Moving old product image to archive when it is no longer in the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8182bc",
   "metadata": {},
   "source": [
    "When there are new updates to the catalogue, it will archive the images so that it will not be included in the recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d39fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir = os.listdir(hafary_img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f742ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_img_path = os.path.join(hafary_img_folder,\"archived\")\n",
    "if not os.path.exists(archive_img_path):\n",
    "    os.makedirs(archive_img_path)  # Create the archive folder if it doesn't exist\n",
    "\n",
    "# Iterate over all files in the image folder\n",
    "for image in listdir:\n",
    "    if os.path.isfile(image):\n",
    "        # Extract the name or identifier from the image filename\n",
    "        image_name = os.path.basename(image)  # Adjust this according to your filename structure\n",
    "\n",
    "        # Check if this image_name exists in the DataFrame\n",
    "        if not any(product_df['Filename'].astype(str).str.contains(image_name)):\n",
    "            # Move the file to the archive folder\n",
    "            try:\n",
    "                shutil.move(os.path.join(hafary_img_folder, image), os.path.join(archive_img_path, image))\n",
    "                print(f'Image moved to archived: {os.path.join(hafary_img_folder, image)}')\n",
    "            except:\n",
    "                print(f'Error: Image not found: {os.path.join(hafary_img_folder, image)}')\n",
    "\n",
    "            print(image_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5991a0f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b429595b",
   "metadata": {},
   "source": [
    "### Next Notebook: [1.3 Scraping Lamitak Website](1.3_web_scraping_lamitak.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
