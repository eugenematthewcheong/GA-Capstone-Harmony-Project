{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adab92c",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project: Harmony\n",
    "## 1.3 Web scraping - Lamitak\n",
    "> Authors: Eugene Matthew Cheong\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29da0f",
   "metadata": {},
   "source": [
    "## Table of Contents ##\n",
    "\n",
    "#### 1. Web Scraping\n",
    "\n",
    "- [1.1 Scraping Lian Seng Hin Website](1.1_web_scraping_liansenghin.ipynb)\n",
    "- [1.2 Scraping Hafary Website](1.2_web_scraping_hafary.ipynb)\n",
    "- [1.3 Scraping Lamitak Website](1.3_web_scraping_lamitak.ipynb)\n",
    "- [1.4 Scraping Nippon Website](1.4_web_scraping_nippon.ipynb)\n",
    "- [1.5 Consolidate All Product Database](1.5_consolidate_product_database.ipynb)\n",
    "\n",
    "#### 2. Preprocessing\n",
    "\n",
    "- [2.1 Processing Canva Palettes](2.1_processing_canva_palette.ipynb)\n",
    "\n",
    "#### 3. Modelling\n",
    "\n",
    "- [3.1 Matching Input Photo to Products](3.1_matching_input_photo_to_products.ipynb)\n",
    "- [3.2 Recommending Canva Palette to Products](3.2_recommending_canva_palette_to_product.ipynb)\n",
    "- [3.3 Recommending Colours and Colour Palettes with Llama3](3.3_recommending_colours_and_colour_palettes_with_llama3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547929d0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b75827",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c994bbbb-74ef-49f6-9823-282217604394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9e6b3",
   "metadata": {},
   "source": [
    "# Website to scrape\n",
    "- https://sg.lamitak.com/collections/woods\n",
    "- https://sg.lamitak.com/collections/solids\n",
    "- https://sg.lamitak.com/collections/patterns\n",
    "- https://sg.lamitak.com/collections/specialities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512e214-26d2-4902-bf8b-7ca12ec1110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_img_folder = \"../datasets/images\"\n",
    "lamitak_img_folder =  os.path.join(data_img_folder,\"lamitak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679f653",
   "metadata": {},
   "source": [
    "## Function to scrape information required per Lamitak page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9caa47-8313-4830-99b0-cf805fac493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape images and labels from a single page\n",
    "def scrape_page(url, design, input_folder):\n",
    "\n",
    "    # Create a directory to store images\n",
    "    os.makedirs(input_folder, exist_ok=True)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find image containers within the specified class\n",
    "    image_containers = soup.find_all('li', class_=\"col-md-3 col-sm-6 col-xs-6 product-item\")\n",
    "\n",
    "    product_list =[]\n",
    "\n",
    "    # Extract image URLs and labels\n",
    "    for container in image_containers:\n",
    "\n",
    "        #print(container)\n",
    "        \n",
    "        pattern = r\"(?<=url\\(').*?(?='\\))\"\n",
    "        image = container.find('a')['style']\n",
    "        image_url = re.search(pattern, image).group()\n",
    "\n",
    "        image_label = container.find('span').text.strip()\n",
    "\n",
    "        product_site = container.find(\"a\", class_=\"product-image\")['href']\n",
    "\n",
    "        image_filename = os.path.splitext(os.path.basename(image_url))[0]\n",
    "\n",
    "        image_dict = {\"Model Name\" : image_label,\n",
    "                      \"Product URL\" : product_site,\n",
    "                      \"Filename\" : f\"{image_filename}.jpg\",\n",
    "                      \"Company\": \"lamitak\",\n",
    "                      \"Type\" : \"Laminate\",\n",
    "                      \"Category Tags\" : f\"{design},\"}\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #Download image and save with label\n",
    "        download_image(image_url, image_filename, input_folder)\n",
    "        product_list.append(image_dict)\n",
    "        print(f\"Image Label Found: {image_label}\")\n",
    "        print(f\"Image Product Site Found: {image_filename}\")\n",
    "        print(f\"Image URL Found: {image_url}\")\n",
    "\n",
    "    return product_list \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683cd674",
   "metadata": {},
   "source": [
    "## Function to download images with given URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9371f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download image and save with label\n",
    "def download_image(url, label, input_folder):\n",
    "    image_data = requests.get(url).content\n",
    "    filename = f\"{label}.jpg\"\n",
    "    image_filepath = os.path.join(input_folder,filename)\n",
    "    with open(image_filepath, 'wb') as f:\n",
    "        f.write(image_data)\n",
    "    print(f\"Image saved: {image_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5717473",
   "metadata": {},
   "source": [
    "## Function to scrape Lian Seng Hin Tile pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a1a945-61dc-4cf5-b7b7-61604c3f77a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to iterate through pages and scrape\n",
    "def lamitak_scrape(base_url_list):\n",
    "\n",
    "    all_product_list = []\n",
    "    design_list = [\"wood\", \"solid-colors\", \"patterns\", \"specialities\"]\n",
    "\n",
    "    for design in design_list:\n",
    "        selectedurl = base_url_list[design]\n",
    "        \n",
    "        print(f\"Scraping page {selectedurl}...\")\n",
    "        product_list_page = scrape_page(selectedurl, design, lamitak_img_folder)\n",
    "        all_product_list += product_list_page\n",
    "\n",
    "    return all_product_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7debc",
   "metadata": {},
   "source": [
    "# Launch scraping for lian Seng Hin tile products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_list = {\"wood\": \"https://sg.lamitak.com/collections/woods\",\n",
    "                 \"solid-colors\": \"https://sg.lamitak.com/collections/solids\",\n",
    "                 \"patterns\": \"https://sg.lamitak.com/collections/patterns\",\n",
    "                 \"specialities\": \"https://sg.lamitak.com/collections/specialities\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bad56d-05c3-47ef-98a8-c31d8f366eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launch scraping for lian seng hin tile products\n",
    "start_time = time.time()\n",
    "\n",
    "all_product_list = lamitak_scrape(base_url_list)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Scraping Runtime:\", runtime, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1029918b",
   "metadata": {},
   "source": [
    "Converting to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.DataFrame(all_product_list)\n",
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df['Category Tags'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe115bd",
   "metadata": {},
   "source": [
    "# Populate Laminate details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878e6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_tile_details(row):\n",
    "   # Export Nippon DF to CSV\n",
    "   product_url = row['Product URL']\n",
    "   print(product_url)\n",
    "\n",
    "   response = requests.get(product_url)\n",
    "   if response.status_code != 200:\n",
    "      print(f\"Failed to fetch {product_url}\")\n",
    "      return\n",
    "\n",
    "   soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "   # Find image containers within the specified class\n",
    "\n",
    "\n",
    "   info_containers = soup.find('div', \"col-md-6 col-xs-12 product-info-container\").find_all('dl', class_= 'product-details-table')\n",
    "\n",
    "   all_dict = {}\n",
    "   # Extract image URLs and labels\n",
    "   for container in info_containers:\n",
    "      product_headers = container.find_all(\"dt\")\n",
    "      product_descriptions = container.find_all(\"dd\")\n",
    "\n",
    "      product_headers_list = []\n",
    "      product_descriptions_list = []\n",
    "\n",
    "      for header in product_headers:\n",
    "         product_headers_list.append(header.text.strip())\n",
    "\n",
    "      for descriptions in product_descriptions:\n",
    "         product_descriptions_list.append(descriptions.text.strip())\n",
    "      \n",
    "      all_dict.update(dict(zip(product_headers_list, product_descriptions_list)))\n",
    "\n",
    "   \n",
    "   product_description = \"\"\n",
    "\n",
    "   try:\n",
    "      product_description += f\"{all_dict['Type']}\"\n",
    "   except:\n",
    "      print(f\"Error: No Type Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "   try:\n",
    "      product_description += f\", {all_dict['Finish']} Finish\"\n",
    "   except:\n",
    "      print(f\"Error: No Features Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "   try:\n",
    "      product_description += f\", {all_dict['Grains']} Grains\"\n",
    "   except:\n",
    "      print(f\"Error: No Features Found for {row['Product URL']}\")\n",
    "      pass\n",
    "\n",
    "   #Procssing dimensions\n",
    "   try:\n",
    "      dimension_text = all_dict['Newedge Size'].replace(\"W\",\"\")\n",
    "      dimension_list = dimension_text.split(\"/\")\n",
    "      row['Width (cm)'] = dimension_list[0]\n",
    "      row['Height (cm)'] = dimension_list[1]\n",
    "   except:\n",
    "      row['Width (cm)'] = \"None\"\n",
    "      row['Height (cm)'] = \"None\"\n",
    "      print(f\"Error: No Measurements Found for {row['Product URL']}\")\n",
    "\n",
    "\n",
    "\n",
    "   row['Origin Country'] = \"None\"\n",
    "   row['Category Tags'] = product_description\n",
    "   row['Application'] = \"Carpentry\"\n",
    "\n",
    "   print(row)\n",
    "   return row\n",
    "\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "product_df =  product_df.apply(populate_tile_details, axis=1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(\"Scraping Runtime:\", runtime, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5a46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b791f5c",
   "metadata": {},
   "source": [
    "# Export Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_dataset_path = \"../datasets/archive_dataset/\"\n",
    "file_path = '../datasets/lamitak_df.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a5a6f",
   "metadata": {},
   "source": [
    "Archives the old csv and updates with the current list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(archive_dataset_path):\n",
    "    os.makedirs(archive_dataset_path)  # Create the archive folder if it doesn't exist\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Move the file to the archive folder\n",
    "    shutil.move(file_path, os.path.join(archive_dataset_path, f\"lamitak_df_archived_{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db66a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936977a5",
   "metadata": {},
   "source": [
    "I noticed after scraping, there are some images that are not correct and showing the tile image. It shows a room instead. So I will update the images later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2afe3",
   "metadata": {},
   "source": [
    "# Find missing files and update to the correct image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_image_list = []\n",
    "\n",
    "for i in list(product_df['Filename']):\n",
    "  full_image_filepath = os.path.join(lamitak_img_folder,i)\n",
    "  if os.path.exists(full_image_filepath):\n",
    "    missing_image_list.append(os.path.join(lamitak_img_folder,i))\n",
    "  else:\n",
    "    print(f\"Error finding image path: {full_image_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833895da",
   "metadata": {},
   "source": [
    "# Moving old product image to archive when it is no longer in the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea737b",
   "metadata": {},
   "source": [
    "When there are new updates to the catalogue, it will archive the images so that it will not be included in the recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73017773",
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir = os.listdir(lamitak_img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b5da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_img_path = os.path.join(lamitak_img_folder,\"archived\")\n",
    "if not os.path.exists(archive_img_path):\n",
    "    os.makedirs(archive_img_path)  # Create the archive folder if it doesn't exist\n",
    "\n",
    "# Iterate over all files in the image folder\n",
    "for image in listdir:\n",
    "    if os.path.isfile(image):\n",
    "        # Extract the name or identifier from the image filename\n",
    "        image_name = os.path.basename(image)  # Adjust this according to your filename structure\n",
    "\n",
    "        # Check if this image_name exists in the DataFrame\n",
    "        if not any(product_df['Filename'].astype(str).str.contains(image_name)):\n",
    "            # Move the file to the archive folder\n",
    "            try:\n",
    "                shutil.move(os.path.join(lamitak_img_folder, image), os.path.join(archive_img_path, image))\n",
    "                print(f'Image moved to archived: {os.path.join(lamitak_img_folder, image)}')\n",
    "            except:\n",
    "                print(f'Error: Image not found: {os.path.join(lamitak_img_folder, image)}')\n",
    "\n",
    "            print(image_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5509766",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caf3fd",
   "metadata": {},
   "source": [
    "### Next Notebook: [1.4 Scraping Nippon Website](1.4_web_scraping_nippon.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
